{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a2e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3b8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config ----------\n",
    "DYS_CSV = \"database/dysgraphia.csv\"\n",
    "NORM_CSV = \"database/normal.csv\"\n",
    "WINDOW_SEC = 2.0\n",
    "WINDOW_STEP_SEC = 1.0  # overlap = WINDOW_SEC - WINDOW_STEP_SEC\n",
    "MODEL_OUT = \"models/dysgraphia_rf_pipeline.pkl\"\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b15cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "def load_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # ensure expected columns exist\n",
    "    expected = [\"timestamp\",\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\",\"p1\",\"p2\"]\n",
    "    missing = [c for c in expected if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in {path}: {missing}\")\n",
    "    # normalize timestamp to seconds (if in ms, convert)\n",
    "    ts = df['timestamp'].values\n",
    "    # if timestamps are large (ms), convert:\n",
    "    if ts.mean() > 1e6:\n",
    "        df['timestamp'] = df['timestamp'] / 1000.0\n",
    "    return df[expected].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71c51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def estimate_fs(df):\n",
    "#     ts = df['timestamp'].values\n",
    "#     diffs = np.diff(ts)\n",
    "#     median_dt = np.median(diffs)\n",
    "#     if median_dt == 0:\n",
    "#         # fallback to 50 Hz\n",
    "#         return 50.0\n",
    "#     return 1.0/median_dt\n",
    "\n",
    "def estimate_fs(df):\n",
    "    \"\"\"Estimate sampling frequency from timestamp deltas.\"\"\"\n",
    "    ts = df['timestamp'].values\n",
    "    diffs = np.diff(ts)\n",
    "    diffs = diffs[diffs > 0]\n",
    "    if len(diffs) == 0:\n",
    "        return 45.0  # fallback\n",
    "    median_dt_ms = np.median(diffs)  # in ms (since 22 ms per sample)\n",
    "    fs = 1000.0 / median_dt_ms\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d33b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sliding_windows(df, window_sec, step_sec):\n",
    "#     fs = estimate_fs(df)\n",
    "#     window_len = int(round(window_sec * fs))\n",
    "#     step = int(round(step_sec * fs))\n",
    "#     array = df[['ax','ay','az','gx','gy','gz','p1','p2']].values\n",
    "#     N = array.shape[0]\n",
    "#     idx = 0\n",
    "#     while idx + window_len <= N:\n",
    "#         yield array[idx: idx+window_len]\n",
    "#         idx += step\n",
    "\n",
    "def sliding_windows(df, window_sec, step_sec):\n",
    "    fs = estimate_fs(df)\n",
    "    window_len = int(round(window_sec * fs))\n",
    "    step = int(round(step_sec * fs))\n",
    "    array = df[['ax','ay','az','gx','gy','gz','p1','p2']].values\n",
    "    N = array.shape[0]\n",
    "\n",
    "    if N < window_len:\n",
    "        print(f\"⚠️ Skipping file: only {N} samples < {window_len} required for one window.\")\n",
    "        return\n",
    "\n",
    "    idx = 0\n",
    "    while idx + window_len <= N:\n",
    "        yield array[idx: idx + window_len]\n",
    "        idx += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3eedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction per window\n",
    "def extract_features(win, fs):\n",
    "    # win: (T, 8) columns ax..p2\n",
    "    features = []\n",
    "    # statistical features for each channel\n",
    "    ch_names = ['ax','ay','az','gx','gy','gz','p1','p2']\n",
    "    for i in range(win.shape[1]):\n",
    "        x = win[:, i]\n",
    "        features += [\n",
    "            x.mean(),\n",
    "            x.std(),\n",
    "            np.min(x),\n",
    "            np.max(x),\n",
    "            np.percentile(x, 25),\n",
    "            np.percentile(x, 50),\n",
    "            np.percentile(x, 75),\n",
    "            np.mean(np.abs(np.diff(x))),  # mean absolute derivative\n",
    "        ]\n",
    "    # magnitude features for accelerometer and gyroscope\n",
    "    acc = np.linalg.norm(win[:, 0:3], axis=1)\n",
    "    gyr = np.linalg.norm(win[:, 3:6], axis=1)\n",
    "    features += [\n",
    "        acc.mean(), acc.std(), acc.max(),\n",
    "        gyr.mean(), gyr.std(), gyr.max()\n",
    "    ]\n",
    "    # frequency-domain: dominant freq for each accel channel\n",
    "    for i in range(3):  # ax ay az\n",
    "        x = win[:, i]\n",
    "        # remove mean\n",
    "        x = x - x.mean()\n",
    "        # compute PSD via Welch\n",
    "        f, Pxx = signal.welch(x, fs=fs, nperseg=min(len(x), 256))\n",
    "        # dominant freq\n",
    "        if Pxx.size>0:\n",
    "            dom = f[np.argmax(Pxx)]\n",
    "            bw = np.sum(Pxx > (Pxx.max() * 0.5))  # crude bandwidth proxy\n",
    "        else:\n",
    "            dom = 0.0\n",
    "            bw = 0.0\n",
    "        features += [dom, bw]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957aa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_dataset(df_list, labels, window_sec, step_sec):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for df, lbl in zip(df_list, labels):\n",
    "#         fs = estimate_fs(df)\n",
    "#         for win in sliding_windows(df, window_sec, step_sec):\n",
    "#             feat = extract_features(win, fs)\n",
    "#             X.append(feat)\n",
    "#             y.append(lbl)\n",
    "#     X = np.vstack(X)\n",
    "#     y = np.array(y)\n",
    "#     return X, y\n",
    "\n",
    "\n",
    "def build_dataset(df_list, labels, window_sec, step_sec):\n",
    "    X = []\n",
    "    y = []\n",
    "    for df, lbl in zip(df_list, labels):\n",
    "        fs = estimate_fs(df)\n",
    "        count = 0\n",
    "        for win in sliding_windows(df, window_sec, step_sec):\n",
    "            if len(win) == 0:\n",
    "                continue\n",
    "            feat = extract_features(win, fs)\n",
    "            X.append(feat)\n",
    "            y.append(lbl)\n",
    "            count += 1\n",
    "        print(f\"✅ Generated {count} windows for label {lbl} ({'Dysgraphia' if lbl==1 else 'Normal'})\")\n",
    "    if not X:\n",
    "        raise ValueError(\"No feature windows generated. Try lowering WINDOW_SEC or verify timestamps.\")\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c791f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading CSVs...\")\n",
    "    dys = load_csv(DYS_CSV)\n",
    "    norm = load_csv(NORM_CSV)\n",
    "    print(f\"dys samples: {len(dys)}, normal samples: {len(norm)}\")\n",
    "\n",
    "    print(\"Building dataset (windowing & features)...\")\n",
    "    X, y = build_dataset([dys, norm], [1, 0], WINDOW_SEC, WINDOW_STEP_SEC)\n",
    "    print(\"Feature matrix shape:\", X.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "    # pipeline with scaler + classifier\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    # quick grid search (optional; you can skip or reduce)\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [100],\n",
    "        'clf__max_depth': [10, 20, None],\n",
    "        'clf__min_samples_split': [2,5]\n",
    "    }\n",
    "\n",
    "    print(\"Training model (GridSearchCV)...\")\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "    best = grid.best_estimator_\n",
    "\n",
    "    y_pred = best.predict(X_test)\n",
    "    print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # save the pipeline\n",
    "    joblib.dump(best, MODEL_OUT)\n",
    "    print(f\"Saved model pipeline to {MODEL_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25308971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs...\n",
      "dys samples: 1647, normal samples: 1317\n",
      "Building dataset (windowing & features)...\n",
      "✅ Generated 35 windows for label 1 (Dysgraphia)\n",
      "✅ Generated 28 windows for label 0 (Normal)\n",
      "Feature matrix shape: (63, 76)\n",
      "Training model (GridSearchCV)...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best params: {'clf__max_depth': 10, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
      "Test accuracy: 1.0\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        13\n",
      "   macro avg       1.00      1.00      1.00        13\n",
      "weighted avg       1.00      1.00      1.00        13\n",
      "\n",
      "Confusion matrix:\n",
      " [[6 0]\n",
      " [0 7]]\n",
      "Saved model pipeline to models/dysgraphia_rf_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497a517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dysgraphia rows: 1647\n",
      "Normal rows: 1317\n",
      "Dysgraphia columns: ['timestamp', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'p1', 'p2']\n",
      "Normal columns: ['timestamp', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'p1', 'p2']\n",
      "   timestamp    ax    ay    az    gx    gy    gz  p1  p2\n",
      "0          0  8.52  5.84 -3.26  0.15  0.23  0.10  21  15\n",
      "1         22  8.32  5.73 -3.38  0.25  0.28  0.10  21   3\n",
      "2         44  6.93  5.89 -4.77 -0.10  0.70  0.79  21  16\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dys = pd.read_csv(\"database/dysgraphia.csv\")\n",
    "norm = pd.read_csv(\"database/normal.csv\")\n",
    "\n",
    "print(\"Dysgraphia rows:\", len(dys))\n",
    "print(\"Normal rows:\", len(norm))\n",
    "print(\"Dysgraphia columns:\", list(dys.columns))\n",
    "print(\"Normal columns:\", list(norm.columns))\n",
    "print(dys.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
